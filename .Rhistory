FROM tweets_gram tn
JOIN conceito c ON c.palavra = tn.palavra
WHERE c.sucesso = 1
GROUP BY 1,
2 ) AS louco
JOIN resource_type ty ON ty.resource = louco.resource
WHERE louco.idTweetInterno = t.idInterno
AND ty.type IN ('http://dbpedia.org/class/yago/Property104916342', 'http://dbpedia.org/class/yago/Manner104928903', 'http://dbpedia.org/class/yago/WikicatBeerStyles', 'http://dbpedia.org/class/yago/Attribute100024264', 'http://dbpedia.org/class/yago/Agent114778436', 'http://dbpedia.org/class/yago/Drug103247620', 'http://dbpedia.org/ontology/Beverage', 'http://dbpedia.org/class/yago/WikicatDrugs', 'http://dbpedia.org/class/yago/Carcinogen114793812', 'http://dbpedia.org/class/yago/WikicatDrugsActingOnTheNervousSystem', 'http://dbpedia.org/class/yago/WikicatIARCGroup1Carcinogens', 'http://dbpedia.org/class/yago/Substance100020090', 'http://dbpedia.org/ontology/Food', 'http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#FunctionalSubstance', 'http://www.wikidata.org/entity/Q2095', 'http://dbpedia.org/class/yago/Matter100020827', 'http://dbpedia.org/class/yago/Abstraction100002137', 'http://dbpedia.org/class/yago/DrugOfAbuse103248958', 'http://dbpedia.org/class/yago/Fluid114939900', 'http://dbpedia.org/class/yago/WikicatDistilledBeverages', 'http://dbpedia.org/class/yago/Liquid114940386', 'http://dbpedia.org/class/yago/CausalAgent100007347', 'http://dbpedia.org/class/yago/Beverage107881800', 'http://dbpedia.org/class/yago/Alcohol107884567', 'http://dbpedia.org/class/yago/Food100021265', 'http://www.w3.org/2002/07/owl#Thing', 'http://umbel.org/umbel/rc/AilmentCondition', 'http://dbpedia.org/class/yago/YagoLegalActorGeo', 'http://dbpedia.org/class/yago/YagoPermanentlyLocatedEntity', 'http://dbpedia.org/class/yago/Substance100019613', 'http://www.wikidata.org/entity/Q12136', 'http://dbpedia.org/ontology/Disease', 'http://dbpedia.org/class/yago/Location100027167', 'http://dbpedia.org/class/yago/Part113809207', 'http://dbpedia.org/class/yago/AdministrativeDistrict108491826', 'http://dbpedia.org/class/yago/YagoGeoEntity', 'http://dbpedia.org/class/yago/District108552138', 'http://dbpedia.org/class/yago/WikicatEthnicGroups', 'http://dbpedia.org/class/yago/Region108630985', 'http://dbpedia.org/ontology/EthnicGroup', 'http://dbpedia.org/class/yago/WikicatEthnicGroupsInTheUnitedStates', 'http://dbpedia.org/class/yago/WikicatEthnicGroupsInCanada', 'http://www.wikidata.org/entity/Q41710', 'http://dbpedia.org/class/yago/Wine107891726', 'http://dbpedia.org/class/yago/WikicatWineStyles', 'http://dbpedia.org/class/yago/SparklingWine107893528', 'http://dbpedia.org/class/yago/WikicatSparklingWines')
) AS entidades
FROM tweets t
WHERE q1 IS NOT NULL
AND id <> 462478714693890048
")
dados <- magica(dados)
dados$numeroErros[dados$numeroErros > 1] <- 1
dados <- discretizarHora(dados)
clearConsole()
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
library(SnowballC)
setDT(dados)
setkey(dados, id)
stem_tokenizer1 =function(x) {
tokens = word_tokenizer(x)
lapply(tokens, SnowballC::wordStem, language="en")
}
dados$textParser = sub("'", "", dados$textParser)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
#                  tokenizer = stem_tokenizer1,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
stop_words = tm::stopwords("en")
vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 2L))
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
it_train = itoken(strsplit(dados$entidades, ","),
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dataFrameResource = create_dtm(it_train, vectorizer)
dataFrameResource <- as.data.frame(as.matrix(dataFrameResource))
#it_train = itoken(strsplit(dados$grams, ","),
#                  ids = dados$id,
#progressbar = TRUE)
#vocab = create_vocabulary(it_train)
#vectorizer = vocab_vectorizer(vocab)
#dataFrameGram = create_dtm(it_train, vectorizer)
#dataFrameGram <- as.data.frame(as.matrix(dataFrameGram))
#Concatenar resultados
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
clearConsole()
library(rowr)
library(RWeka)
maFinal <- cbind.fill(dados, dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- cbind.fill(maFinal, dataFrameResource)
maFinal <- subset(maFinal, select = -c(textParser, id, hashtags, textoCompleto, entidades))
save(maFinal, file = "2110/rdas/2-Gram-dbpedia-types-information-gain-hora-erro-not-null.Rda")
load("2110/rdas/compare_artigo_exaustivo.RData")
load("2110/rdas/compare_artigo_exaustivo.RData")
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
source(file_path_as_absolute("processadores/discretizar.R"))
DATABASE <- "icwsm"
clearConsole();
dados <- query("SELECT t.id,
q1 AS resposta,
textParser,
textoParserEmoticom AS textoCompleto,
hashtags,
emoticonPos,
emoticonNeg,
(SELECT GROUP_CONCAT(tn.palavra)
FROM semantic_tweets_nlp tn
WHERE tn.idTweet = t.id
GROUP BY tn.idTweet) AS entidades
FROM semantic_tweets t
WHERE textparser <> ''
")
dados <- query("SELECT t.id,
textParser,
textoParserEmoticom AS textoCompleto,
hashtags,
emoticonPos,
emoticonNeg,
(SELECT GROUP_CONCAT(tn.palavra)
FROM semantic_tweets_nlp tn
WHERE tn.idTweet = t.id
GROUP BY tn.idTweet) AS entidades
FROM semantic_tweets t
WHERE textparser <> ''
")
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
source(file_path_as_absolute("processadores/discretizar.R"))
DATABASE <- "icwsm"
clearConsole();
dados <- query("SELECT t.id,
textParser,
textOriginal AS textoCompleto,
hashtags,
emoticonPos,
emoticonNeg,
(SELECT GROUP_CONCAT(tn.palavra)
FROM semantic_tweets_nlp tn
WHERE tn.idTweet = t.id
GROUP BY tn.idTweet) AS entidades
FROM semantic_tweets t
WHERE textparser <> ''
")
dados$textParser <- enc2utf8(dados$textParser)
dados$textParser <- iconv(dados$textParser, to='ASCII//TRANSLIT')
dados$hashtags = gsub("#", "#tag_", dados$hashtags)
clearConsole()
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
library(SnowballC)
setDT(dados)
setkey(dados, id)
stem_tokenizer1 =function(x) {
tokens = word_tokenizer(x)
lapply(tokens, SnowballC::wordStem, language="en")
}
dados$textParser = gsub("'", "", dados$textParser)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
#                  tokenizer = stem_tokenizer1,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
stop_words = tm::stopwords("en")
vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 2L))
dtm_train_texto = create_dtm(it_train, vectorizer)
vectorizer = vocab_vectorizer(vocab)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
it_train = itoken(strsplit(dados$entidades, ","),
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dataFrameEntidades = create_dtm(it_train, vectorizer)
dataFrameEntidades <- as.data.frame(as.matrix(dataFrameEntidades))
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
clearConsole()
library(rowr)
library(RWeka)
maFinal <- cbind.fill(dados, dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- cbind.fill(maFinal, dataFrameEntidades)
resultados <- data.frame(matrix(ncol = 4, nrow = 0))
names(resultados) <- c("Baseline", "F1", "Precisão", "Revocação")
try({
load("mardigras/mardigras_compare.RData")
})
library(tools)
library(caret)
if (!require("doMC")) {
install.packages("doMC")
}
library(doMC)
library(mlbench)
CORES <- 10
registerDoMC(CORES)
treinar <- function(data_train){
fit <- train(x = subset(data_train, select = -c(resposta)),
y = data_train$resposta,
method = "svmLinear",
trControl = trainControl(method = "cv", number = 5, savePred=T))
return (fit)
}
getMatriz <- function(fit, data_test) {
pred <- predict(fit, subset(data_test, select = -c(resposta)))
matriz <- confusionMatrix(data = pred, data_test$resposta, positive="1")
return (matriz)
}
addRow <- function(resultados, baseline, matriz, ...) {
print(baseline)
newRes <- data.frame(baseline, matriz$byClass["F1"], matriz$byClass["Precision"], matriz$byClass["Recall"])
rownames(newRes) <- baseline
names(newRes) <- c("Baseline", "F1", "Precisão", "Revocação")
newdf <- rbind(resultados, newRes)
return (newdf)
}
library(magrittr)
set.seed(10)
split=0.80
if (!exists("matriz2GramEntidades")) {
try({
load("mardigras/2gram-entidades.Rda")
pred <- predict(twoGramEntidadesHoraErroNotNull, subset(maFinal, select = -c(resposta)))
#matriz <- confusionMatrix(data = pred, data_test$resposta, positive="1")
})
}
load("mardigras/2gram-entidades.Rda")
pred <- predict(twoGramEntidadesHoraErroNotNull, maFinal)
View(maFinal)
pred <- predict(twoGramEntidadesHoraErroNotNull, newData = maFinal)
View(maFinal)
pred
load("mardigras/2gram-entidades.Rda")
pred <- predict(twoGramEntidadesHoraErroNotNull, newData = maFinal)
View(maFinal)
nrow(maFinal)
pred <- predict(twoGramEntidadesHoraErroNotNull, newData = maFinal)
pred
nrow(pred)
nrow(maFinal)
predNew <- predict(twoGramEntidadesHoraErroNotNull, newData = maFinal)
nrow(pred)
View(predNew)
predNew
dump(predNew, "mardigras/pred.csv")
nrow(maFinal)
teste <- maFinal
predNew <- predict(twoGramEntidadesHoraErroNotNull, newData = teste)
predNew
twoGramEntidadesHoraErroNotNull
nrow(twoGramEntidadesHoraErroNotNull)
resultados
predNew <- predict(twoGramEntidadesHoraErroNotNull, newData = resultados)
predNew
predNew <- predict(twoGramEntidadesHoraErroNotNull, newData = head(resultados))
predNew
bothModels <- list(knn = twoGramEntidadesHoraErroNotNull)
predict(bothModels)
extractPrediction(bothModels, maFinal)
View(extractPrediction(bothModels, maFinal))
load("2110/rdas/2gram-entidades-hora-erro-not-null.Rda")
maFinal$resposta <- as.factor(maFinal$resposta)
trainIndex <- createDataPartition(maFinal$resposta, p=split, list=FALSE)
data_train <- as.data.frame(unclass(maFinal[ trainIndex,]))
data_test <- maFinal[-trainIndex,]
nrow(data_test)
predNew <- predict(twoGramEntidadesHoraErroNotNull, newData = data_test)
View(predNew)
predNew
predNew <- predict(twoGramEntidadesHoraErroNotNull, data_test)
predNew
predNew <- predict(twoGramEntidadesHoraErroNotNull, subset(data_test, select = -c(resposta)))
predNew
predNew <- predict(twoGramEntidadesHoraErroNotNull, newData = subset(data_test, select = -c(resposta)))
predNew
predNew <- predict(twoGramEntidadesHoraErroNotNull, maFinal)
predNew
load("mardigras/2gram-entidades.Rda")
predNew <- predict(twoGramEntidadesHoraErroNotNull, maFinal)
load("2110/rdas/2gram-entidades-hora-erro-not-null.Rda")
antigo <- maFinal
predNew <- predict(twoGramEntidadesHoraErroNotNull, newData = subset(antigo, select = -c(resposta, drunk)))
predNew
predict(twoGramEntidadesHoraErroNotNull, newData = subset(antigo, select = -c(resposta, drunk)))
nrow(antigo)
predict(twoGramEntidadesHoraErroNotNull, newData = subset(antigo, select = -c(resposta)))
nrow(antigo)
predict(twoGramEntidadesHoraErroNotNull, newData = subset(antigo, select = -c(resposta)))
predict(twoGramEntidadesHoraErroNotNull, newData = subset(antigo, select = -c(resposta)))
predict(twoGramEntidadesHoraErroNotNull, subset(antigo, select = -c(resposta)))
nrow(antigo)
predict(twoGramEntidadesHoraErroNotNull, subset(antigo, select = -c(resposta)))
predict(twoGramEntidadesHoraErroNotNull, subset(data_test, select = -c(resposta)))
load("2110/rdas/2gram-entidades-hora-erro-not-null.Rda")
antigo <- maFinal
predict(twoGramEntidadesHoraErroNotNull, subset(antigo, select = -c(resposta)))
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
source(file_path_as_absolute("processadores/discretizar.R"))
#Configuracoes
DATABASE <- "icwsm"
clearConsole();
dados <- query("SELECT t.id,
q1 AS resposta,
textParser,
textoParserEmoticom AS textoCompleto,
hashtags,
emoticonPos,
emoticonNeg,
hora,
erroParseado as numeroErros,
(SELECT GROUP_CONCAT(tn.palavra)
FROM tweets_nlp tn
WHERE tn.idTweetInterno = t.idInterno
GROUP BY tn.idTweetInterno) AS entidades
FROM tweets t
WHERE textparser <> ''
AND id <> 462478714693890048
AND q1 IS NOT NULL")
dados$resposta[is.na(dados$resposta)] <- 0
dados$resposta <- as.factor(dados$resposta)
dados$textParser <- enc2utf8(dados$textParser)
dados$numeroErros[dados$numeroErros > 1] <- 1
dados$textParser <- iconv(dados$textParser, to='ASCII//TRANSLIT')
dados$hashtags = gsub("#", "#tag_", dados$hashtags)
dados <- discretizarHora(dados)
clearConsole()
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
library(SnowballC)
setDT(dados)
setkey(dados, id)
stem_tokenizer1 =function(x) {
tokens = word_tokenizer(x)
lapply(tokens, SnowballC::wordStem, language="en")
}
dados$textParser = sub("'", "", dados$textParser)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
#                  tokenizer = stem_tokenizer1,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
stop_words = tm::stopwords("en")
vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 2L))
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
it_train = itoken(strsplit(dados$entidades, ","),
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dataFrameEntidades = create_dtm(it_train, vectorizer)
dataFrameEntidades <- as.data.frame(as.matrix(dataFrameEntidades))
#it_train = itoken(strsplit(dados$grams, ","),
#                  ids = dados$id,
#progressbar = TRUE)
#vocab = create_vocabulary(it_train)
#vectorizer = vocab_vectorizer(vocab)
#dataFrameGram = create_dtm(it_train, vectorizer)
#dataFrameGram <- as.data.frame(as.matrix(dataFrameGram))
#Concatenar resultados
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
clearConsole()
library(rowr)
library(RWeka)
maFinal <- cbind.fill(dados, dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- cbind.fill(maFinal, dataFrameEntidades)
maFinal <- subset(maFinal, select = -c(textParser, id, hashtags, textoCompleto, entidades))
save(maFinal, file = "2110/rdas/2gram-entidades-hora-erro-not-null-new-test.Rda")
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
source(file_path_as_absolute("processadores/discretizar.R"))
#Configuracoes
DATABASE <- "icwsm"
clearConsole();
dados <- query("SELECT t.id,
q1 AS resposta,
textParser,
textoParserEmoticom AS textoCompleto,
hashtags,
emoticonPos,
emoticonNeg,
hora,
erroParseado as numeroErros,
(SELECT GROUP_CONCAT(tn.palavra)
FROM tweets_nlp tn
WHERE tn.idTweetInterno = t.idInterno
GROUP BY tn.idTweetInterno) AS entidades
FROM tweets t
WHERE textparser <> ''
AND id <> 462478714693890048
AND q1 IS NOT NULL")
dados$resposta[is.na(dados$resposta)] <- 0
dados$resposta <- as.factor(dados$resposta)
dados$textParser <- enc2utf8(dados$textParser)
dados$numeroErros[dados$numeroErros > 1] <- 1
dados$textParser <- iconv(dados$textParser, to='ASCII//TRANSLIT')
dados$hashtags = gsub("#", "#tag_", dados$hashtags)
dados <- discretizarHora(dados)
clearConsole()
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
library(SnowballC)
setDT(dados)
setkey(dados, id)
stem_tokenizer1 =function(x) {
tokens = word_tokenizer(x)
lapply(tokens, SnowballC::wordStem, language="en")
}
dados$textParser = sub("'", "", dados$textParser)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
#                  tokenizer = stem_tokenizer1,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
stop_words = tm::stopwords("en")
vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 2L))
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
it_train = itoken(strsplit(dados$entidades, ","),
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dataFrameEntidades = create_dtm(it_train, vectorizer)
dataFrameEntidades <- as.data.frame(as.matrix(dataFrameEntidades))
#it_train = itoken(strsplit(dados$grams, ","),
#                  ids = dados$id,
#progressbar = TRUE)
#vocab = create_vocabulary(it_train)
#vectorizer = vocab_vectorizer(vocab)
#dataFrameGram = create_dtm(it_train, vectorizer)
#dataFrameGram <- as.data.frame(as.matrix(dataFrameGram))
#Concatenar resultados
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
clearConsole()
library(rowr)
library(RWeka)
maFinal <- cbind.fill(dados, dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- cbind.fill(maFinal, dataFrameEntidades)
maFinal <- subset(maFinal, select = -c(textParser, id, hashtags, textoCompleto, entidades))
save(maFinal, file = "2110/rdas/2gram-entidades-hora-erro-not-null-new-test.Rda")
load("2110/rdas/2gram-entidades-hora-erro-not-null-new-test.Rda")
nrow(maFinal)
predict(twoGramEntidadesHoraErroNotNull, subset(maFinal,    select = -c(resposta)))
load("mardigras/mardigras_compare.RData")
load("2110/rdas/2gram-entidades-hora-erro-not-null-new-test.Rda")
predict(twoGramEntidadesHoraErroNotNull, subset(maFinal,    select = -c(resposta)))
load("2110/rdas/2gram-entidades-hora-erro-not-null-new-test.Rda")
nrow(maFinal)
predict(twoGramEntidadesHoraErroNotNull, subset(maFinal, select = -c(resposta)))
load("mardigras/mardigras_compare.RData")
resultados <- data.frame(matrix(ncol = 4, nrow = 0))
names(resultados) <- c("Baseline", "F1", "Precisão", "Revocação")
load("mardigras/mardigras_compare.RData")
ls()
treegram25
predict(twoGramEntidadesHoraErroNotNull, subset(maFinal, select = -c(resposta)))
predict(treegram25, subset(maFinal, select = -c(resposta)))
load("2110/rdas/2gram-entidades-hora-erro-not-null-new-test.Rda")
predict(treegram25, subset(maFinal, select = -c(resposta)))
rm("treegram25")
save.image(file = "mardigras/mardigras_compare.RData")
load("2110/rdas/2gram-entidades-hora-erro-not-null-new-test.Rda")
parte1 <- subset(maFinal, select = -c(drunk, drink, bacardi, emoticonPos))
parte2 <- subset(maFinal, select = c(drunk, drink, bacardi, emoticonPos))
library(rowr)
library(RWeka)
maFinalTwo <- cbind.fill(parte1, parte2)
save(maFinalTwo, file="mardigras/2gram-entidades-hora-erro-not-null-new-test_two.Rda")
View(maFinalTwo)
colnames(maFinalTwo)
