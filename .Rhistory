dados$adverbio[dados$taxaAdverbio > 0.34] <- 2
dados$adverbio[dados$taxaAdverbio > 0.51] <- 3
dados$adverbio[dados$taxaAdverbio > 0.68] <- 4
dados$verbo <- 0
dados$verbo[dados$taxaVerbo > 0.17] <- 1
dados$verbo[dados$taxaVerbo > 0.34] <- 2
dados$verbo[dados$taxaVerbo > 0.51] <- 3
dados$verbo[dados$taxaVerbo > 0.68] <- 4
#}
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
library(SnowballC)
setDT(dados)
setkey(dados, id)
stem_tokenizer1 =function(x) {
tokens = word_tokenizer(x)
lapply(tokens, SnowballC::wordStem, language="en")
}
dados$textParser = sub("'", "", dados$textParser)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
#                  tokenizer = stem_tokenizer1,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
stop_words = tm::stopwords("en")
vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 3L))
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
dataTexto <- as.matrix(dtm_train_texto)
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
clearConsole()
library(rowr)
#summary(dados$sentiment)
#dados$teste<-cut(dados$sentiment, seq(-2,2,0.5))
#summary(dados$teste)
library(RWeka)
#table(discretize(dados$sentiment, categories=3))
#bb <- function() {
#sentimentos
dados$emotiom <- 0
dados$emotiom[dados$sentiment < 0] <- -1
dados$emotiom[dados$sentiment < -0.33] <- -2
dados$emotiom[dados$sentiment < -0.66] <- -3
dados$emotiom[dados$sentiment > 0] <- 1
dados$emotiom[dados$sentiment > 0.33] <- 2
dados$emotiom[dados$sentiment > 0.66] <- 3
dados$emotiomH <- 0
dados$emotiomH[dados$sentimentH < 0] <- -1
dados$emotiomH[dados$sentimentH < -0.5] <- -2
dados$emotiomH[dados$sentimentH > 0] <- 1
dados$emotiomH[dados$sentimentH > 0.5] <- 2
#}
cols <- colnames(dataFrameTexto)
aspectos <- sort(colSums(dataFrameTexto), decreasing = TRUE)
manter <- round(length(aspectos) * 0.25)
aspectosManter <- c()
aspectosRemover <- c()
for(i in 1:length(aspectos)) {
if (i <= manter) {
aspectosManter <- c(aspectosManter, aspectos[i])
} else {
aspectosRemover <- c(aspectosRemover, aspectos[i])
}
}
dataFrameTexto <- dataFrameTexto[names(aspectosManter)]
testea <- function() {
library(tm)
corpus <- Corpus(VectorSource(sub("#", "HASH_", sub("#", "HASH_",dados$textParser))))
#corpus <- Corpus(VectorSource(dataFrameTexto))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, stopwords)
#funcs <- list(tolower)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(1, Inf)))
findFreqTerms(a.dtm1, 5000)
wordFreq <- function(corpus, word) {
results <- lapply(corpus,
function(x) { grep(as.character(x), pattern=paste0("\\<",word)) }
)
sum(unlist(results))
}
wordFreq(corpus, "beer")
findFreqTerms(a.dtm1, 50)
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train_texto, tfidf)
# tfidf modified by fit_transform() call!
# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf  = create_dtm(it_train, vectorizer) %>%
transform(tfidf)
dtm_test_tfidf
}
maFinal <- cbind.fill(dados, dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- subset(maFinal, select = -c(textParser, id, hashtags, textoCompleto))
maFinal <- subset(maFinal, select = -c(sentiment, sentimentH))
maFinal <- subset(maFinal, select = -c(taxaAdjetivo, taxaAdverbio, taxaSubstantivo, taxaVerbo))
#maFinal <- subset(maFinal, select = -c(emotiom, emotiomH))
#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, adjetivo, verbo))
#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, adjetivo, verbo))
maFinal <- subset(maFinal, select = -c(adverbio, substantivo, verbo))
#maFinal <- subset(maFinal, select = -c(emotiomH, emotiom, organizationCount, personCount, localCount, moneyCount))
#maFinal <- subset(maFinal, select = -c(organizationCount, personCount, localCount, moneyCount))
save(maFinal, file = "dados_0108.Rda")
options(max.print = 99999999)
#Carrega functions
library(tools)
source(file_path_as_absolute("functions.R"))
#Configuracoes
DATABASE <- "icwsm-2016"
clearConsole();
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes, palavroes FROM tweets WHERE situacao = 'S'")
#dadosDele <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes FROM tweets WHERE situacao = 'S'")
dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q2 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S' AND q1 = 1")
dados <- dadosQ1
dados$resposta[is.na(dados$resposta)] <- 0
dados$numeroErros[dados$numeroErros > 1] <- 1
dados$palavroes[dados$palavroes > 1] <- 1
dados$resposta <- as.factor(dados$resposta)
clearConsole()
#aa <- function() {
dados$adjetivo <- 0
dados$adjetivo[dados$taxaAdjetivo > 0.20] <- 1
dados$adjetivo[dados$taxaAdjetivo > 0.40] <- 2
dados$adjetivo[dados$taxaAdjetivo > 0.60] <- 3
dados$adjetivo[dados$taxaAdjetivo > 0.80] <- 4
dados$substantivo <- 0
dados$substantivo[dados$taxaSubstantivo > 0.15] <- 1
dados$substantivo[dados$taxaSubstantivo > 0.30] <- 2
dados$substantivo[dados$taxaSubstantivo > 0.45] <- 3
dados$substantivo[dados$taxaSubstantivo > 0.60] <- 4
dados$substantivo[dados$taxaSubstantivo > 0.75] <- 5
dados$substantivo[dados$taxaSubstantivo > 0.90] <- 6
dados$adverbio <- 0
dados$adverbio[dados$taxaAdverbio > 0.17] <- 1
dados$adverbio[dados$taxaAdverbio > 0.34] <- 2
dados$adverbio[dados$taxaAdverbio > 0.51] <- 3
dados$adverbio[dados$taxaAdverbio > 0.68] <- 4
dados$verbo <- 0
dados$verbo[dados$taxaVerbo > 0.17] <- 1
dados$verbo[dados$taxaVerbo > 0.34] <- 2
dados$verbo[dados$taxaVerbo > 0.51] <- 3
dados$verbo[dados$taxaVerbo > 0.68] <- 4
#}
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
library(SnowballC)
setDT(dados)
setkey(dados, id)
stem_tokenizer1 =function(x) {
tokens = word_tokenizer(x)
lapply(tokens, SnowballC::wordStem, language="en")
}
dados$textParser = sub("'", "", dados$textParser)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
#                  tokenizer = stem_tokenizer1,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
stop_words = tm::stopwords("en")
vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 3L))
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
dataTexto <- as.matrix(dtm_train_texto)
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
clearConsole()
library(rowr)
#summary(dados$sentiment)
#dados$teste<-cut(dados$sentiment, seq(-2,2,0.5))
#summary(dados$teste)
library(RWeka)
#table(discretize(dados$sentiment, categories=3))
#bb <- function() {
#sentimentos
dados$emotiom <- 0
dados$emotiom[dados$sentiment < 0] <- -1
dados$emotiom[dados$sentiment < -0.33] <- -2
dados$emotiom[dados$sentiment < -0.66] <- -3
dados$emotiom[dados$sentiment > 0] <- 1
dados$emotiom[dados$sentiment > 0.33] <- 2
dados$emotiom[dados$sentiment > 0.66] <- 3
dados$emotiomH <- 0
dados$emotiomH[dados$sentimentH < 0] <- -1
dados$emotiomH[dados$sentimentH < -0.5] <- -2
dados$emotiomH[dados$sentimentH > 0] <- 1
dados$emotiomH[dados$sentimentH > 0.5] <- 2
#}
cols <- colnames(dataFrameTexto)
aspectos <- sort(colSums(dataFrameTexto), decreasing = TRUE)
manter <- round(length(aspectos) * 0.25)
aspectosManter <- c()
aspectosRemover <- c()
for(i in 1:length(aspectos)) {
if (i <= manter) {
aspectosManter <- c(aspectosManter, aspectos[i])
} else {
aspectosRemover <- c(aspectosRemover, aspectos[i])
}
}
dataFrameTexto <- dataFrameTexto[names(aspectosManter)]
testea <- function() {
library(tm)
corpus <- Corpus(VectorSource(sub("#", "HASH_", sub("#", "HASH_",dados$textParser))))
#corpus <- Corpus(VectorSource(dataFrameTexto))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, stopwords)
#funcs <- list(tolower)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(1, Inf)))
findFreqTerms(a.dtm1, 5000)
wordFreq <- function(corpus, word) {
results <- lapply(corpus,
function(x) { grep(as.character(x), pattern=paste0("\\<",word)) }
)
sum(unlist(results))
}
wordFreq(corpus, "beer")
findFreqTerms(a.dtm1, 50)
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train_texto, tfidf)
# tfidf modified by fit_transform() call!
# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf  = create_dtm(it_train, vectorizer) %>%
transform(tfidf)
dtm_test_tfidf
}
maFinal <- cbind.fill(dados, dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- subset(maFinal, select = -c(textParser, id, hashtags, textoCompleto))
maFinal <- subset(maFinal, select = -c(sentiment, sentimentH))
maFinal <- subset(maFinal, select = -c(taxaAdjetivo, taxaAdverbio, taxaSubstantivo, taxaVerbo))
#maFinal <- subset(maFinal, select = -c(emotiom, emotiomH))
#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, adjetivo, verbo))
#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, adjetivo, verbo))
#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, verbo))
#maFinal <- subset(maFinal, select = -c(emotiomH, emotiom, organizationCount, personCount, localCount, moneyCount))
#maFinal <- subset(maFinal, select = -c(organizationCount, personCount, localCount, moneyCount))
save(maFinal, file = "dados_0108.Rda")
options(max.print = 99999999)
#Carrega functions
library(tools)
source(file_path_as_absolute("functions.R"))
#Configuracoes
DATABASE <- "icwsm-2016"
clearConsole();
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes, palavroes FROM tweets WHERE situacao = 'S'")
#dadosDele <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes FROM tweets WHERE situacao = 'S'")
dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q2 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S' AND q1 = 1")
dados <- dadosQ1
dados$resposta[is.na(dados$resposta)] <- 0
dados$numeroErros[dados$numeroErros > 1] <- 1
dados$palavroes[dados$palavroes > 1] <- 1
dados$resposta <- as.factor(dados$resposta)
clearConsole()
#aa <- function() {
dados$adjetivo <- 0
dados$adjetivo[dados$taxaAdjetivo > 0.20] <- 1
dados$adjetivo[dados$taxaAdjetivo > 0.40] <- 2
dados$adjetivo[dados$taxaAdjetivo > 0.60] <- 3
dados$adjetivo[dados$taxaAdjetivo > 0.80] <- 4
dados$substantivo <- 0
dados$substantivo[dados$taxaSubstantivo > 0.15] <- 1
dados$substantivo[dados$taxaSubstantivo > 0.30] <- 2
dados$substantivo[dados$taxaSubstantivo > 0.45] <- 3
dados$substantivo[dados$taxaSubstantivo > 0.60] <- 4
dados$substantivo[dados$taxaSubstantivo > 0.75] <- 5
dados$substantivo[dados$taxaSubstantivo > 0.90] <- 6
dados$adverbio <- 0
dados$adverbio[dados$taxaAdverbio > 0.17] <- 1
dados$adverbio[dados$taxaAdverbio > 0.34] <- 2
dados$adverbio[dados$taxaAdverbio > 0.51] <- 3
dados$adverbio[dados$taxaAdverbio > 0.68] <- 4
dados$verbo <- 0
dados$verbo[dados$taxaVerbo > 0.17] <- 1
dados$verbo[dados$taxaVerbo > 0.34] <- 2
dados$verbo[dados$taxaVerbo > 0.51] <- 3
dados$verbo[dados$taxaVerbo > 0.68] <- 4
#}
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
library(SnowballC)
setDT(dados)
setkey(dados, id)
stem_tokenizer1 =function(x) {
tokens = word_tokenizer(x)
lapply(tokens, SnowballC::wordStem, language="en")
}
dados$textParser = sub("'", "", dados$textParser)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
#                  tokenizer = stem_tokenizer1,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
stop_words = tm::stopwords("en")
vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 3L))
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
dataTexto <- as.matrix(dtm_train_texto)
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
clearConsole()
library(rowr)
#summary(dados$sentiment)
#dados$teste<-cut(dados$sentiment, seq(-2,2,0.5))
#summary(dados$teste)
library(RWeka)
#table(discretize(dados$sentiment, categories=3))
#bb <- function() {
#sentimentos
dados$emotiom <- 0
dados$emotiom[dados$sentiment < 0] <- -1
dados$emotiom[dados$sentiment < -0.33] <- -2
dados$emotiom[dados$sentiment < -0.66] <- -3
dados$emotiom[dados$sentiment > 0] <- 1
dados$emotiom[dados$sentiment > 0.33] <- 2
dados$emotiom[dados$sentiment > 0.66] <- 3
dados$emotiomH <- 0
dados$emotiomH[dados$sentimentH < 0] <- -1
dados$emotiomH[dados$sentimentH < -0.5] <- -2
dados$emotiomH[dados$sentimentH > 0] <- 1
dados$emotiomH[dados$sentimentH > 0.5] <- 2
#}
cols <- colnames(dataFrameTexto)
aspectos <- sort(colSums(dataFrameTexto), decreasing = TRUE)
manter <- round(length(aspectos) * 0.25)
aspectosManter <- c()
aspectosRemover <- c()
for(i in 1:length(aspectos)) {
if (i <= manter) {
aspectosManter <- c(aspectosManter, aspectos[i])
} else {
aspectosRemover <- c(aspectosRemover, aspectos[i])
}
}
dataFrameTexto <- dataFrameTexto[names(aspectosManter)]
testea <- function() {
library(tm)
corpus <- Corpus(VectorSource(sub("#", "HASH_", sub("#", "HASH_",dados$textParser))))
#corpus <- Corpus(VectorSource(dataFrameTexto))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, stopwords)
#funcs <- list(tolower)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(1, Inf)))
findFreqTerms(a.dtm1, 5000)
wordFreq <- function(corpus, word) {
results <- lapply(corpus,
function(x) { grep(as.character(x), pattern=paste0("\\<",word)) }
)
sum(unlist(results))
}
wordFreq(corpus, "beer")
findFreqTerms(a.dtm1, 50)
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train_texto, tfidf)
# tfidf modified by fit_transform() call!
# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf  = create_dtm(it_train, vectorizer) %>%
transform(tfidf)
dtm_test_tfidf
}
maFinal <- cbind.fill(dados, dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- subset(maFinal, select = -c(textParser, id, hashtags, textoCompleto))
maFinal <- subset(maFinal, select = -c(sentiment, sentimentH))
#maFinal <- subset(maFinal, select = -c(taxaAdjetivo, taxaAdverbio, taxaSubstantivo, taxaVerbo))
#maFinal <- subset(maFinal, select = -c(emotiom, emotiomH))
#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, adjetivo, verbo))
#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, adjetivo, verbo))
#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, verbo))
#maFinal <- subset(maFinal, select = -c(emotiomH, emotiom, organizationCount, personCount, localCount, moneyCount))
#maFinal <- subset(maFinal, select = -c(organizationCount, personCount, localCount, moneyCount))
save(maFinal, file = "dados_0108.Rda")
options(max.print = 99999999)
#Carrega functions
library(tools)
source(file_path_as_absolute("functions.R"))
#Configuracoes
DATABASE <- "icwsm-2016"
clearConsole();
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes, palavroes FROM tweets WHERE situacao = 'S'")
#dadosDele <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, numeroErros, numeroConjuncoes FROM tweets WHERE situacao = 'S'")
dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'")
#dadosQ1 <- query("SELECT id, q2 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S' AND q1 = 1")
dados <- dadosQ1
dados$resposta[is.na(dados$resposta)] <- 0
dados$numeroErros[dados$numeroErros > 1] <- 1
#dados$palavroes[dados$palavroes > 1] <- 1
dados$resposta <- as.factor(dados$resposta)
clearConsole()
#aa <- function() {
dados$adjetivo <- 0
dados$adjetivo[dados$taxaAdjetivo > 0.20] <- 1
dados$adjetivo[dados$taxaAdjetivo > 0.40] <- 2
dados$adjetivo[dados$taxaAdjetivo > 0.60] <- 3
dados$adjetivo[dados$taxaAdjetivo > 0.80] <- 4
dados$substantivo <- 0
dados$substantivo[dados$taxaSubstantivo > 0.15] <- 1
dados$substantivo[dados$taxaSubstantivo > 0.30] <- 2
dados$substantivo[dados$taxaSubstantivo > 0.45] <- 3
dados$substantivo[dados$taxaSubstantivo > 0.60] <- 4
dados$substantivo[dados$taxaSubstantivo > 0.75] <- 5
dados$substantivo[dados$taxaSubstantivo > 0.90] <- 6
dados$adverbio <- 0
dados$adverbio[dados$taxaAdverbio > 0.17] <- 1
dados$adverbio[dados$taxaAdverbio > 0.34] <- 2
dados$adverbio[dados$taxaAdverbio > 0.51] <- 3
dados$adverbio[dados$taxaAdverbio > 0.68] <- 4
dados$verbo <- 0
dados$verbo[dados$taxaVerbo > 0.17] <- 1
dados$verbo[dados$taxaVerbo > 0.34] <- 2
dados$verbo[dados$taxaVerbo > 0.51] <- 3
dados$verbo[dados$taxaVerbo > 0.68] <- 4
#}
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
library(SnowballC)
setDT(dados)
setkey(dados, id)
stem_tokenizer1 =function(x) {
tokens = word_tokenizer(x)
lapply(tokens, SnowballC::wordStem, language="en")
}
dados$textParser = sub("'", "", dados$textParser)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
#                  tokenizer = stem_tokenizer1,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
stop_words = tm::stopwords("en")
vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 3L))
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
dataTexto <- as.matrix(dtm_train_texto)
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
