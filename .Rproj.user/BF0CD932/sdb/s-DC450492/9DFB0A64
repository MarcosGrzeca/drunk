{
    "collab_server" : "",
    "contents" : "options(max.print = 99999999)\n\n#Carrega functions\nlibrary(tools)\nsource(file_path_as_absolute(\"functions.R\"))\n\n#Configuracoes\nDATABASE <- \"icwsm-2016\"\nclearConsole();\n\ndados <- query(\"SELECT id, q1 as resposta, textParser, textoParserEmoticom as textoCompleto, hashtags, emoticonPos, emoticonNeg, sentiment, sentimentH, localCount, organizationCount, moneyCount, personCount, numeroErros, numeroConjuncoes, taxaSubstantivo, taxaAdjetivo, taxaAdverbio, taxaVerbo, palavroes FROM tweets WHERE situacao = 'S'\")\n\ndados$resposta[is.na(dados$resposta)] <- 0\ndados$numeroErros[dados$numeroErros > 1] <- 1\ndados$palavroes[dados$palavroes > 1] <- 1\ndados$resposta <- as.factor(dados$resposta)\nclearConsole()\n\n\n#aa <- function() {\n  dados$adjetivo <- 0\n  dados$adjetivo[dados$taxaAdjetivo > 0.20] <- 1\n  dados$adjetivo[dados$taxaAdjetivo > 0.40] <- 2\n  dados$adjetivo[dados$taxaAdjetivo > 0.60] <- 3\n  dados$adjetivo[dados$taxaAdjetivo > 0.80] <- 4\n  \n  dados$substantivo <- 0\n  dados$substantivo[dados$taxaSubstantivo > 0.15] <- 1\n  dados$substantivo[dados$taxaSubstantivo > 0.30] <- 2\n  dados$substantivo[dados$taxaSubstantivo > 0.45] <- 3\n  dados$substantivo[dados$taxaSubstantivo > 0.60] <- 4\n  dados$substantivo[dados$taxaSubstantivo > 0.75] <- 5\n  dados$substantivo[dados$taxaSubstantivo > 0.90] <- 6\n  \n  dados$adverbio <- 0\n  dados$adverbio[dados$taxaAdverbio > 0.17] <- 1\n  dados$adverbio[dados$taxaAdverbio > 0.34] <- 2\n  dados$adverbio[dados$taxaAdverbio > 0.51] <- 3\n  dados$adverbio[dados$taxaAdverbio > 0.68] <- 4\n  \n  dados$verbo <- 0\n  dados$verbo[dados$taxaVerbo > 0.17] <- 1\n  dados$verbo[dados$taxaVerbo > 0.34] <- 2\n  dados$verbo[dados$taxaVerbo > 0.51] <- 3\n  dados$verbo[dados$taxaVerbo > 0.68] <- 4\n#}\n\n\nif (!require(\"text2vec\")) {\n  install.packages(\"text2vec\")\n}\nlibrary(text2vec)\nlibrary(data.table)\nlibrary(SnowballC)\n\nsetDT(dados)\nsetkey(dados, id)\n\nstem_tokenizer1 =function(x) {\n  tokens = word_tokenizer(x)\n  lapply(tokens, SnowballC::wordStem, language=\"en\")\n}\n\ndados$textParser = sub(\"'\", \"\", dados$textParser)\n\nprep_fun = tolower\ntok_fun = word_tokenizer\n\nit_train = itoken(dados$textParser, \n                  preprocessor = prep_fun, \n                  #                  tokenizer = stem_tokenizer1,\n                  tokenizer = tok_fun,\n                  ids = dados$id, \n                  progressbar = TRUE)\n\nstop_words = tm::stopwords(\"en\")\nvocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L, 3L))\nvectorizer = vocab_vectorizer(vocab)\ndtm_train_texto = create_dtm(it_train, vectorizer)\n\nit_train_hash = itoken(dados$hashtags, \n                       preprocessor = prep_fun, \n                       tokenizer = tok_fun, \n                       ids = dados$id, \n                       progressbar = TRUE)\n\nvocabHashTags = create_vocabulary(it_train_hash)\nvectorizerHashTags = vocab_vectorizer(vocabHashTags)\ndtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)\n\ndataTexto <- as.matrix(dtm_train_texto)\ndataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))\ndataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))\nclearConsole()\n\nlibrary(rowr)\n\n#summary(dados$sentiment)\n#dados$teste<-cut(dados$sentiment, seq(-2,2,0.5))\n#summary(dados$teste)\n\nlibrary(RWeka)\n#table(discretize(dados$sentiment, categories=3))\n\n\n#bb <- function() {\n  \n  #sentimentos\n  dados$emotiom <- 0\n  dados$emotiom[dados$sentiment < 0] <- -1\n  dados$emotiom[dados$sentiment < -0.33] <- -2\n  dados$emotiom[dados$sentiment < -0.66] <- -3\n  dados$emotiom[dados$sentiment > 0] <- 1\n  dados$emotiom[dados$sentiment > 0.33] <- 2\n  dados$emotiom[dados$sentiment > 0.66] <- 3\n  \n  dados$emotiomH <- 0\n  dados$emotiomH[dados$sentimentH < 0] <- -1\n  dados$emotiomH[dados$sentimentH < -0.5] <- -2\n  dados$emotiomH[dados$sentimentH > 0] <- 1\n  dados$emotiomH[dados$sentimentH > 0.5] <- 2\n#}\n\ncols <- colnames(dataFrameTexto)\naspectos <- sort(colSums(dataFrameTexto), decreasing = TRUE)\nmanter <- round(length(aspectos) * 0.25)\naspectosManter <- c()\naspectosRemover <- c()\n\nfor(i in 1:length(aspectos)) {\n  if (i <= manter) {\n    aspectosManter <- c(aspectosManter, aspectos[i])\n  } else {\n    aspectosRemover <- c(aspectosRemover, aspectos[i])\n  }\n}\n\ndataFrameTexto <- dataFrameTexto[names(aspectosManter)]\n\ntestea <- function() {\n  library(tm)\n  \n  corpus <- Corpus(VectorSource(sub(\"#\", \"HASH_\", sub(\"#\", \"HASH_\",dados$textParser))))\n  #corpus <- Corpus(VectorSource(dataFrameTexto))\n  funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, stopwords)\n  #funcs <- list(tolower)\n  a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)\n  a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(1, Inf))) \n  findFreqTerms(a.dtm1, 5000)\n  \n  wordFreq <- function(corpus, word) {\n    results <- lapply(corpus,\n                      function(x) { grep(as.character(x), pattern=paste0(\"\\\\<\",word)) }\n    )\n    sum(unlist(results))\n  }\n  \n  wordFreq(corpus, \"beer\")\n  findFreqTerms(a.dtm1, 50)\n  \n  \n  tfidf = TfIdf$new()\n  # fit model to train data and transform train data with fitted model\n  dtm_train_tfidf = fit_transform(dtm_train_texto, tfidf)\n  # tfidf modified by fit_transform() call!\n  # apply pre-trained tf-idf transformation to test data\n  dtm_test_tfidf  = create_dtm(it_train, vectorizer) %>% \n    transform(tfidf)\n  dtm_test_tfidf\n}\n\nmaFinal <- cbind.fill(dados, dataFrameTexto)\nmaFinal <- cbind.fill(maFinal, dataFrameHash)\n#Sempre remover\nmaFinal <- subset(maFinal, select = -c(textParser, id, hashtags, textoCompleto))\nmaFinal <- subset(maFinal, select = -c(sentiment, sentimentH))\nmaFinal <- subset(maFinal, select = -c(taxaAdjetivo, taxaAdverbio, taxaSubstantivo, taxaVerbo))\n\n#Sob demanda\n#maFinal <- subset(maFinal, select = -c(emotiom, emotiomH))\n#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, adjetivo, verbo))\n#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, adjetivo, verbo))\n#maFinal <- subset(maFinal, select = -c(adverbio, substantivo, verbo))\n#maFinal <- subset(maFinal, select = -c(emotiomH, emotiom, organizationCount, personCount, localCount, moneyCount))\n#maFinal <- subset(maFinal, select = -c(organizationCount, personCount, localCount, moneyCount))\nsave(maFinal, file = \"dados_0108.Rda\")\n\n#exp1.Rda\n#exp1_bag.Rda\n#exp1_bag_sentiment.Rda\n#exp1_completao.Rda\n#exp1_stemming.Rda\nFILE <- \"exp1_completao.Rda\"\n\n#save(maFinal, file=FILE)\n#load(FILE)\n\nload(\"dados_0108.Rda\")\n\nlibrary(tools)\nlibrary(caret)\n\nif (!require(\"doMC\")) {\n  install.packages(\"doMC\")\n}\nlibrary(doMC)\n\nregisterDoMC(8)\n\nset.seed(10)\nsplit=0.80\ntrainIndex <- createDataPartition(maFinal$resposta, p=split, list=FALSE)\ndata_train <- as.data.frame(unclass(maFinal[ trainIndex,]))\ndata_test <- maFinal[-trainIndex,]\n\nprint(\"Treinando\")\nfit <- train(x = subset(data_train, select = -c(resposta)),\n             y = data_train$resposta, \n             method = \"svmLinear\", \n             trControl = trainControl(method = \"cv\", number = 10, savePred=T)\n             #,preProc=c(\"center\", \"scale\", \"nzv\")\n             ,preProc=c(\"center\")\n) \nfit\n\nlibrary(mlbench)\n\n#subset(data_test, select = -c(resposta))\npred <- predict(fit, subset(data_test, select = -c(resposta)))\nconfusionMatrix(data = pred, data_test$resposta, positive=\"1\")\n\n#fitPadrao <- fit\n\nload(\"tentando.Rda\")\nlibrary(caret)\n\nresamps <- resamples(list(\"3-GRAM, #, Emoticon, CV\" = fitTrigram,\n                          \"BoW, #, Emoticon, CV\" = fitBagWords,\n                          \"BoW, #, Emoticon, Sentiment, Sentiment #, CV\" = fitBagWordsSentiment,\n                          \"BoW, #, Emoticon, Sentiment, Sentiment #, NLP, CV\" = fitCompletao,\n                          \"3-GRAM, #, Emoticon, Stemming, CV\" = fitStemming,\n                          \"BoW, #, Emoticon, Sentiment, Sentiment #, NLP\" = fitSemCV))\n\nsummary(resamps)\n\nbwplot(resamps, layout = c(1, 2))\n\ntrellis.par.set(caretTheme())\ndotplot(resamps, metric = \"Accuracy\")\n\ntrellis.par.set(theme1)\nxyplot(resamps, what = \"BlandAltman\")\n\ndifValues <- diff(resamps)\ntrellis.par.set(theme1)\nbwplot(difValues, layout = c(3, 1))\n\ndotplot(difValues)\n\n\npred <- predict(fitSemCV, subset(data_test, select = -c(resposta)))\nconfusionMatrix(data = pred, data_test$resposta, positive=\"1\")\n",
    "created" : 1501634300013.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "146354830",
    "id" : "9DFB0A64",
    "lastKnownWriteTime" : 1501675114,
    "last_content_update" : 1501675114182,
    "path" : "~/GitHub/drunk/exp1.R",
    "project_path" : "exp1.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}